# 第七章 **深度学习中的正则化**

> 正则化：旨在减少泛化误差，而不是训练误差，用于显示地被设计来减少测试误差(可能会以增大训练误差为代价)

## **参数范数惩罚**

> 许多正则化方法通过对目标函数J添加一个参数范数惩罚Ω(θ)，限制模型（如神经网络、线性回归或逻辑回归）的学习能力。

$$\widetilde {J}\left( \theta ;x,y\right) =J\left( \theta ;X,y\right) +\alpha \Omega \left( \theta \right) $$

α越大，对应正则化惩罚越大。在神经网络中，参数包括每一层的仿射变换的权重和偏置，通常**只对**权重做惩罚而不对偏置做正则惩罚。每个权重会指定两个变量如何相互作用，需要在各种条件下观察这两个变量才能良好地拟合权重。而每个偏置仅控制一个单变量。这意味着，不对其进行正则化也不会导致太大的方差。另外，正则化偏置参数可能会导致明显的**欠拟合**。因此，使用向量w表示所有应受范数惩罚影响的权重，而向量θ表示所有参数(包括w和无需正则化的参数)。

#### $L^{2}$参数正则化

> 权重衰减（weight decay）:通过向目标函数添加一个正则项
>
> Ω(θ) =$\dfrac {1}{2}\left\| w\right\| ^{2}_{2}$ 使权重更加接近原点

加入权重衰减后会引起学习规则的修改，即在每步执行通常的梯度更新之前先收缩权重向量

## 作为约束的范数惩罚

> 如果我们想约束Ω(θ)小于某个常数k，我们可以构建广义Lagrange函数

$L\left( \theta ,\alpha ;x,y\right)=J\left( \theta ;X,y\right) +\alpha \left( \Omega \left( \theta \right) -k\right) $

## 数据集增强

> 让机器学习模型泛化得更好的最好办法是使用更多的数据进行训

- 沿训练图像每个方向平移几个像素的操作通常可以大大改善泛化。许多其他操作如旋转图像或缩放图像也已被证明非常有效。

- 在神经网络的输入层注入噪声也可以被看作是数据增强的一种方式;噪声的幅度被细心调整后,也是非常高效的。

## 噪声鲁棒性

一种正则化模型的噪声使用方式是将其加到权重,这项技术主要用于循环神经网络,可以被解释为关于权重的**贝叶斯推断**的随机实现。贝叶斯学习过程将权重视为不确定的，并且可以通过概率分布表示这种不确定性。向权重添加噪声是反映这种不确定性的一种实用的随机方法。

## 半监督学习

> 在半监督学习的框架下，P(x)产生的未标记样本和P(x,y)中的标记样本都用于估计$P(  x| y)$ 或者根据x预测y














