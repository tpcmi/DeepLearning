# 深度模型中的优化

## 学习和纯优化有什么不同

- **机器学习通常是间接作用的**。在大多数机器学习问题中，我们关注某些性能度量P，其定义于测试集上并且可能是不可解的,因此，我们只是间接地优化P.
- J(θ) =$E\left( x,y\right) \sim\widehat{p}{{data}}L\left( f\left( x;\theta \right) ,y\right)$

#### 经验风险最小化

机器学习算法的目标是降低上式所示的期望泛化误差。但是通常我们是不知道真实分布pdata(x,y)的，将机器学习问题转化回一个优化问题的最简单方法是最小化训练集上的期望损失。这意味着用训练集上的经验分布$\widehat {p}$(x, y)替代真实分布p(x,y)。

![1566695812914](C:\Users\Codemao\AppData\Roaming\Typora\typora-user-images\1566695812914.png)

#### 代理损失函数和提前终止

> 当损失函数不能高效地优化可以通过优化**代理函数**（作为原目标的代理）

## 神经网络中的优化挑战



