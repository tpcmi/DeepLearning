# 第七章 **深度学习中的正则化**

> 正则化：旨在减少泛化误差，而不是训练误差，用于显示地被设计来减少测试误差(可能会以增大训练误差为代价)

## **参数范数惩罚**

> 许多正则化方法通过对目标函数J添加一个参数范数惩罚Ω(θ)，限制模型（如神经网络、线性回归或逻辑回归）的学习能力。

$$\widetilde {J}\left( \theta ;x,y\right) =J\left( \theta ;X,y\right) +\alpha \Omega \left( \theta \right) $$

α越大，对应正则化惩罚越大。在神经网络中，参数包括每一层的仿射变换的权重和偏置，通常**只对**权重做惩罚而不对偏置做正则惩罚。每个权重会指定两个变量如何相互作用，需要在各种条件下观察这两个变量才能良好地拟合权重。而每个偏置仅控制一个单变量。这意味着，不对其进行正则化也不会导致太大的方差。另外，正则化偏置参数可能会导致明显的**欠拟合**。因此，使用向量w表示所有应受范数惩罚影响的权重，而向量θ表示所有参数(包括w和无需正则化的参数)。

#### $L^{2}$参数正则化

> 权重衰减（weight decay）:通过向目标函数添加一个正则项
>
> Ω(θ) =$\dfrac {1}{2}\left\| w\right\| ^{2}_{2}$ 使权重更加接近原点

加入权重衰减后会引起学习规则的修改，即在每步执行通常的梯度更新之前先收缩权重向量

## 作为约束的范数惩罚

> 如果我们想约束Ω(θ)小于某个常数k，我们可以构建广义Lagrange函数

$L\left( \theta ,\alpha ;x,y\right)=J\left( \theta ;X,y\right) +\alpha \left( \Omega \left( \theta \right) -k\right) $

## 数据集增强

> 让机器学习模型泛化得更好的最好办法是使用更多的数据进行训

- 沿训练图像每个方向平移几个像素的操作通常可以大大改善泛化。许多其他操作如旋转图像或缩放图像也已被证明非常有效。

- 在神经网络的输入层注入噪声也可以被看作是数据增强的一种方式;噪声的幅度被细心调整后,也是非常高效的。

## 噪声鲁棒性

一种正则化模型的噪声使用方式是将其加到权重,这项技术主要用于循环神经网络,可以被解释为关于权重的**贝叶斯推断**的随机实现。贝叶斯学习过程将权重视为不确定的，并且可以通过概率分布表示这种不确定性。向权重添加噪声是反映这种不确定性的一种实用的随机方法。

## 半监督学习

> 在半监督学习的框架下，P(x)产生的未标记样本和P(x,y)中的标记样本都用于估计$P(  x| y)$ 或者根据x预测y

## 多任务学习

> 通过合并几个任务中的样例（可以视为对参数施加的软约束）来提高泛化的一种方式

## 提前终止

>当验证集上的误差在事先指定的循环次数内没有进一步改善时，算法就会终止

## 参数绑定与参数共享

> 有两个模型执行相同的分类任务，但输入分布稍有不同，某些参数应当彼此接近

对于如下两个任务相似的式子

$\widehat {y}\left( A\right) =f\left( W\left( A\right) ,x\right)$ , $\widehat {y}\left( B\right) =f\left( W\left( B\right) ,x\right)$

- 可以使用参数范数惩罚,迫使彼此接近：
 $\Omega \left( W^{\left( A\right) },W^{\left( B\right) }\right)$  =$\left\| W^{\left( A\right) }-W^{\left( B\right) }\right\| ^{2}_{2}$
- 强迫某些参数相等--**参数共享**：只有唯一一个参数集合需要被存储在内存中，对于某些特定模型，如卷积神经网络，这可能可以显著减少模型所占用的内存。

## 稀疏表示

> 前文所述的权重衰减直接惩罚模型参数。另一种策略是惩罚神经网络中的激活单元，稀疏化激活单元.这种策略间接地对模型参数施加了复杂惩罚。

- $L^{1}$惩罚诱导参数稀疏性
- 通过激活值的硬性约束来获得表示稀疏

## Bagging和其它集成方法

> Bagging:通过结合几个模型降低泛化误差的技术.主要想法是分别训练几个不同的模型，然后让所有模型表决测试样例的输出

- 模型平均(**model averaging**):奏效的原因是不同的模型通常不会在测试集上产生完全相同的误差

  Bagging涉及构造k个不同的数据集。每个数据集从原始数据集中重复采样构成，和原始数据集具有相同数量的样例。这意味着，每个数据集以高概率缺少一些来自原始数据集的例子，还包含若干重复的例子.如果成员的误差是独立的，集成将显著地比其成员表现得更好。
  
  神经网络能找到足够多的不同的解，意味着他们可以从**模型平均**中受益(即使所有模型都在同一数据集上训练)。

## Dropout

> 集成大量深层神经网络的实用Bagging方法

Bagging涉及训练多个模型，并在每个测试样本上评估多个模型，当模型是一个很大的神经网络的时候，会花费大量运行时间和内存（通常5、6个）。

dropout提供了廉价的Bagging集成近似，训练由所有子网络组成的集成![1566374336316](C:\Users\Codemao\AppData\Roaming\Typora\typora-user-images\1566374336316.png)

**Bagging和Dropout训练不太一样**：

- Bagging所有模型都是独立的，Dropout所有模型共享参数，其中每个模型继承父神经网络参数的不同子集。参数共享使得在有限可用的内存下表示指数级数量的模型变得可能。

- 在Bagging的情况下，每一个模型在其相应训练集上训练到收敛。在Dropout的情况下，通常大部分模型都没有显式地被训练，因为通常父神经网络会很大，以致于到宇宙毁灭都不可能采样完所有的子网络。取而代之的是，在单个步骤中我们训练一小部分的子网络，参数共享会使得剩余的子网络也能有好的参数设定。

  这些是仅有的区别。除了这些，Dropout与Bagging算法一样。例如，每个子网络中遇到的训练集确实是有放回采样的原始训练集的一个子集。

## 对抗训练

> 人类观察者不会察觉原始样本和对抗样本（adversarial example）之间的差异，但是网络会作出非常不同的预测,可以通过对抗训练（adversarial training）减少原有独立同分布的测试集的错误率——在对抗扰动的训练集样本上训练网络

对抗训练通过鼓励网络在训练数据附近的<u>局部区域恒定</u>来限制这一高度敏感的局部线性行为。这可以被看作是一种明确地向监督神经网络引入局部恒定先验的方法。

对抗训练有助于体现积极正则化与大型函数族结合的力量。纯粹的线性模型，如逻辑回归，由于它们被限制为线性而无法抵抗对抗样本。神经网络能够将函数从接近线性转化为局部近似恒定，从而可以灵活地捕获到训练数据中的线性趋势同时学习抵抗局部扰动。

对抗样本也提供了一种实现半监督学习的方法。

## 切面距离、正切传播和流形正切分类器

> 许多机器学习通过假设数据位于低维流形附近来克服维数灾难

一种流形假设早期尝试是切面距离（tangent distance）算法。是一种非参数的近邻算法，使用的度量不是通用的欧几里得距离，而是根据邻近流形关于聚集概率的知识导出的。这个算法假设我们将尝试分类的样本和同一流形上的样本具有相同的类别。由于分类器应该对局部因素（对于流形上的移动）的变化保持不变。

正切传播（tangent prop）算法训练带有额外惩罚的神经网络分类器，使神经网络的每个输出

![img](https://ask.qcloudimg.com/http-save/yehe-2965868/xewy38l1aa.gif)

对已知的变化因素是局部不变的。这些变化因素对应于沿着相同样本聚集的流形的移动。正切传播算法不仅用于监督学习，还在强化学习中有所应用。



![img](https://ask.qcloudimg.com/raw/yehe-b339206accf59/swi97d3d0y.png?imageView2/2/w/1620)

正切传播与数据集增强密切相关，在这两种情况下，该算法的用户通过制定一组应当不会改变网络输出的转换，将其先验知识编码至算法中，不同的是在数据集增强的情况下，网络显式地训练正确分类这些施加大量变换后产生的不同输入。正切传播不需要显式访问一个新的输入点。而是解析地对模型正则化从而在指定转换方向抵抗扰动。有两个主要缺点：首先，模型的正则化只能抵抗无穷小的扰动。显式的数据集增强能抵抗较大的扰动。其次，很难在基于整流线性单元的模型上使用无线小的方法。这些模型只能通过关闭单元或缩小它们的权重才能缩小它们的导数。数据集增强在整流线性单元上工作的很好，因为不同的整理单元会在每一个原始输入的不同转换版本上被激活。

正切传播也和双反向传播以及对抗训练有关联。双反向传播正则化是Jacobian矩阵偏小，而对抗训练找到原输入附近的点，训练模型在这些点上产生于与原来输入相同的输出。正切传播和手动指定转换的数据集增强都要求模型在输入变化的某些特定方向上报纸不变。双反向传播和对抗训练都要求模型对输入所有方向中的变化（只要该变化较小）都 应当保持不变。数据集增强是正切传播非无限小的版本，对抗训练是双反向传播非无限小的版本。












